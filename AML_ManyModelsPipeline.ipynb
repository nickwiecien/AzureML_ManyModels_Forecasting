{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c686c277",
   "metadata": {},
   "source": [
    "# Azure Machine Learning - Many Models Training/Forecasting Pipeline Creation\n",
    "This notebook demonstrates creation and execution of an Azure ML pipeline designed to load bulk time-series data from an AML-linked datastore, split the data into individual time-series and register as a File Dataset, train a forecasting model for each distinct time-series using AutoML, generate a forward-looking forecast for each distinct time-series, and finally aggregate all forecasted results and register them as a dataset in the AML workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5772c81",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1bdb6c",
   "metadata": {},
   "source": [
    "### Connect to Azure ML workspace, provision compute resources, and get references to datastores\n",
    "Connect to workspace using config associated config file. Get a reference to you pre-existing AML compute cluster or provision a new cluster to facilitate processing. Finally, get references to your default blob datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ac720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "compute_target_name = 'cpucluster'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_target_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D13_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=10)\n",
    "    compute_target = ComputeTarget.create(ws, compute_target_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "    \n",
    "#Get default datastore\n",
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97652af",
   "metadata": {},
   "source": [
    "### Create Run Configuration\n",
    "The `RunConfiguration` defines the environment used across all python steps. You can optionally add additional conda or pip packages to be added to your environment. [More details here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py).\n",
    "Here, we also register the environment to the AML workspace so that it can be used for future retraining and inferencing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment = Environment(name='many_models_env')\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create()\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['sklearn', 'pandas', 'joblib', 'azureml-defaults', 'azureml-core', 'azureml-dataprep[fuse]'])\n",
    "run_config.environment.python.conda_dependencies.set_python_version('3.8.10')\n",
    "\n",
    "#Register environment for reuse \n",
    "run_config.environment.register(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec252e",
   "metadata": {},
   "source": [
    "### Define Output Datasets\n",
    "Below we define the configuration for datasets that will be passed between steps in our pipeline. Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the register_on_complete() call. In this example, we also define column types to support AutoML operations during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b112ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_dataset = OutputFileDatasetConfig(name='oj_forecast_data', destination=(ds, 'oj_forecast_data/{run-id}')).register_on_complete(name='oj_forecast_data')\n",
    "train_dataset = OutputFileDatasetConfig(name='oj_train_data',destination=(ds, 'oj_train_data/{run-id}')).register_on_complete(name='oj_train_data')\n",
    "result_dataset = OutputFileDatasetConfig(name='oj_result_data',destination=(ds, 'oj_result_data/{run-id}')).read_delimited_files().register_on_complete(name='oj_result_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc1643",
   "metadata": {},
   "source": [
    "### Define Pipeline Parameters\n",
    "`PipelineParameter` objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Below we define the following parameters for our Azure ML Pipeline:\n",
    "\n",
    "| Parameter Name  | Parameter Description |\n",
    "|------------- | -------------|\n",
    "|`source_dataset_name`  | The name of the bulk time-series dataset available in the AML workspace. |\n",
    "|`group_column_names`  | Semicolon-delimited list of column names which uniquely identify individual time-series.|\n",
    "|`timestamp_column`  | Name of the column which contains timestamps.|\n",
    "|`cutoff_date`  | Final date in the dataset to be included in forecast model training. All datapoints after the cutoff date will be included in the `forecast_dataset`.|\n",
    "\n",
    "<b><i>Modification Note:</i></b> When leveraging this sample for your own forecasting activities, update the default values of all columns to reflect your registered dataset, group columns, timestamp column, and cutoff date within your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset_name = PipelineParameter(name='source_dataset_name', default_value='OJ-Sales-Data')\n",
    "group_column_names = PipelineParameter(name='group_column_names', default_value='Store;Brand')\n",
    "timestamp_column = PipelineParameter(name='timestamp_column', default_value='WeekStarting')\n",
    "cutoff_date = PipelineParameter(name='cutoff_date', default_value='1992-05-28')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371243a8",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps\n",
    "The pipeline below consists of four distinct steps to prepare data, train models, generate forecasts, and aggregate results. First, we call `organize_data.py` and retrieve data from the registered datastore, split into individual time-series according to the columns listed in `group_column_names`, then further separate into training and forecasting subsets based on the specified `cutoff_date`, save each time-series to a file and register as a new File Dataset. \n",
    "\n",
    "From here we configure an AutoML job forecasting job which will train a model for each distinct time-series (using it's associated training data) and register into the workspace. For efficiency, these models are trained in parallel across multiple nodes in a compute cluster. To reduce training/inferencing time, increase the number of nodes in your cluster.\n",
    "\n",
    "Following training, we generate a forecast for each individual time-series across the dates included in the `forecast_dataset` using each time-series' best-performing model.\n",
    "\n",
    "Finally, we aggregate all of the forecasted results across time-series into a single dataset (`result_dataset`) and register that in the AML datastore.\n",
    "\n",
    "Each component is broken out individually and described in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421dad6e",
   "metadata": {},
   "source": [
    "### Organize Data (Pipeline Step)\n",
    "The step below executes the script included in `pipeline_step_scripts/organize_data.py` and separates out individual time series into test/inference subsets that get saved into multiple files.\n",
    "\n",
    "<b><i>Modification Note:</i></b> If you need to extrapolate dates further into the future to support forward looking forecasting, the underlying code can be modified to generate dates that extend into the future. [See this document for reference](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PythonScriptStep to gather data from remote source and register as AML dataset\n",
    "organize_data_step = PythonScriptStep(\n",
    "    name='Organize Time-Series Data',\n",
    "    script_name=\"organize_data.py\", \n",
    "    arguments=[\"--train_dataset\", train_dataset, \n",
    "               \"--forecast_dataset\", forecast_dataset, \n",
    "               \"--source_dataset_name\", source_dataset_name, \n",
    "               '--group_column_names', group_column_names,\n",
    "              '--timestamp_column', timestamp_column,\n",
    "              '--cutoff_date', cutoff_date],\n",
    "    outputs=[train_dataset, forecast_dataset],\n",
    "    compute_target=compute_target, \n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969c315",
   "metadata": {},
   "source": [
    "### Configure and Run Many Models AutoML Training Job (Pipeline Step)\n",
    "The code below configures the settings for your AutoML job (`automl_settings`). Details on these settings can be found [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train). Once configured, a `ParallelRunStep` is created to execute training jobs for each individual time-series with an appropriate configured environment. Helper functions located in the `automl_train/scripts` dir are leveraged here.\n",
    "\n",
    "<b><i>Modification Note:</i></b> The following variabels need be updated inside of `automl_settings` when configuring your own forecasting job: `label_column_name`, `time_column_name`, `group_column_names`, and `grain_column_names`. The latter two should include all of the columns which can uniquely identify a particular time-series. To reduce training times, consider increasing the `node_count` and `process_count_per_node` variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf10030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from automl_train.scripts.helper import write_automl_settings_to_file, build_parallel_run_config\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "\n",
    "#Set up AutoML configuration and write to a file\n",
    "automl_settings = {\n",
    "    \"task\" : 'forecasting',\n",
    "    \"primary_metric\" : 'normalized_root_mean_squared_error',\n",
    "    \"iteration_timeout_minutes\" : 60, # This needs to be changed based on the dataset. We ask customer to explore how long training is taking before settings this value\n",
    "    \"iterations\" : 15,\n",
    "    \"experiment_timeout_hours\" : 3,\n",
    "    \"label_column_name\" : 'Quantity',\n",
    "    \"n_cross_validations\" : 3,\n",
    "    \"verbosity\" : logging.INFO, \n",
    "    \"debug_log\": 'automl_sales_debug.txt',\n",
    "    \"time_column_name\": 'WeekStarting',\n",
    "    \"max_horizon\" : 20,\n",
    "    \"track_child_runs\": False,\n",
    "    \"group_column_names\": ['Store', 'Brand'],\n",
    "    \"grain_column_names\": ['Store', 'Brand']\n",
    "}\n",
    "write_automl_settings_to_file(automl_settings)\n",
    "\n",
    "#Set up training environment (reused for inferencing later)\n",
    "from dev.automl_train.scripts.helper import get_automl_environment\n",
    "train_env = get_automl_environment(workspace=ws, automl_settings_dict=automl_settings)\n",
    "\n",
    "#Configure your cluster\n",
    "node_count=5\n",
    "process_count_per_node=8\n",
    "run_invocation_timeout=3700\n",
    "\n",
    "#Build parallel run step configuration\n",
    "parallel_run_config = build_parallel_run_config(train_env, compute_target, node_count, process_count_per_node, run_invocation_timeout)\n",
    "training_output_name = \"training_output\"\n",
    "train_output_dir = PipelineData(name=training_output_name, \n",
    "                          datastore=ds)\n",
    "\n",
    "from azureml.pipeline.steps import ParallelRunStep\n",
    "\n",
    "#Define training ParallelRunStep\n",
    "train_parallel_run_step = ParallelRunStep(\n",
    "    name=\"Many Models Training (AutoML)\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    allow_reuse = False,\n",
    "    inputs=[train_dataset.as_input(name='train_data')],\n",
    "    output=train_output_dir,\n",
    ")\n",
    "\n",
    "#Specify that training step much occur after data gathering step\n",
    "train_parallel_run_step.run_after(organize_data_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281e725",
   "metadata": {},
   "source": [
    "### Configure and Run Many Models AutoML Forecasting Job (Pipeline Step)\n",
    "The code below configures a parallel forecasting job for each individual time-series contained within `forecast_dataset`. \n",
    "\n",
    "<b><i>Modification Note:</i></b> When modifying for your own forecasting activites, update the `--group_column_names`, `--time_column_name`, and `--target_column_name` arguments in the `ParallelRunStep` definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Pipeline Step for Inferencing\n",
    "forecast_env = train_env\n",
    "\n",
    "from automl_inference.scripts.helper import build_parallel_run_config_for_forecasting\n",
    "\n",
    "#Set up configuration for parallel inferencing run\n",
    "node_count=5\n",
    "process_count_per_node=10\n",
    "run_invocation_timeout=300 # this timeout(in seconds), for larger models need to change this to a higher timeout\n",
    "\n",
    "parallel_run_config = build_parallel_run_config_for_forecasting(forecast_env, compute_target, node_count, process_count_per_node, run_invocation_timeout)\n",
    "\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import ParallelRunStep\n",
    "\n",
    "#Define location where forecasting output will be saved\n",
    "forecasting_output_name = 'automl_forecasting_output'\n",
    "forecast_output_dir = PipelineData(name = forecasting_output_name, \n",
    "                          datastore = ds)\n",
    "\n",
    "#Create parallel inferencing step\n",
    "inference_parallel_run_step = ParallelRunStep(\n",
    "    name=\"Many Models Forecasting\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[forecast_dataset.as_input(name='inference_data')], \n",
    "    output=forecast_output_dir,\n",
    "    arguments=[\n",
    "              '--append_row_dataframe_header', True,\n",
    "              '--group_column_names', 'Store', 'Brand',\n",
    "              '--time_column_name', 'WeekStarting', #[Optional] # this is needed for timeseries\n",
    "              '--target_column_name', 'Quantity', # [Optional] Needs to be passed only if inference data contains target column.\n",
    "              ])\n",
    "\n",
    "#Specify that inferencing must happen after training\n",
    "inference_parallel_run_step.run_after(train_parallel_run_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a0c5a",
   "metadata": {},
   "source": [
    "### Aggregate and Save Forecasted Results (Pipeline Step)\n",
    "The code below executes executes the script at `./pipeline_step_scripts/format_and_save_results.py` designed to aggregate and format forecasted results before registering as a new dataset (`result_dataset`) in the default AML datastore.\n",
    "\n",
    "<b><i>Modification Note:</i></b> If you desire to land your forecasted results in a location other than AML, the script referenced above can be modified to sink data in other locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_results_step = PythonScriptStep(\n",
    "    name='Format and Save Forecasting Results',\n",
    "    script_name=\"format_and_save_results.py\", \n",
    "    arguments=[\"--result_dataset\", result_dataset, \n",
    "               \"--forecast_output_dir\", forecast_output_dir],\n",
    "    inputs=[forecast_output_dir],\n",
    "    outputs=[result_dataset],\n",
    "    compute_target=compute_target, \n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "format_results_step.run_after(inference_parallel_run_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823ef7f",
   "metadata": {},
   "source": [
    "### Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b74b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[organize_data_step, train_parallel_run_step, inference_parallel_run_step,  format_results_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41018719",
   "metadata": {},
   "source": [
    "### Trigger a Pipeline Execution from the Notebook\n",
    "You can create an Experiment (logical collection for runs) and submit a pipeline run directly from this notebook by running the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, 'many-models-pipeline-run')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
